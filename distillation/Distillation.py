# -*- coding: utf-8 -*-
"""KnowledgeDistillation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JhVH-Ugv_tWabYJCKG7ejCnFATlocDxw
"""

import numpy as np
import torch
import wandb
from tqdm import tqdm
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset

teacher_model_path = "tabularisai/multilingual-sentiment-analysis"
student_model_path = "tabularisai/robust-sentiment-analysis"

teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_model_path)
student_model = AutoModelForSequenceClassification.from_pretrained(student_model_path)

teacher_model_tokenizer = AutoTokenizer.from_pretrained(teacher_model_path)
student_model_tokenizer = AutoTokenizer.from_pretrained(student_model_path)

sum(p.numel() for p in student_model.parameters())/1e6

"""# **Datasets**"""

teacher_model_tokenizer

dataset = load_dataset("yassiracharki/Yelp_Reviews_for_Sentiment_Analysis_fine_grained_5_classes")

train_datasets = dataset['train'].select(range(12000))
valid_datasets = dataset['train'].select(range(5000))
test_datasets = dataset['test'].select(range(5000))

class MyCustomDataset(Dataset):

    def __init__(self, dataset):
        self.input = dataset['review_text']
        self.label = dataset['class_index']
        self.teacher_tokenizer = teacher_model_tokenizer
        self.student_tokenizer = student_model_tokenizer

    def __len__(self):
        return len(self.input)

    def __getitem__(self, idx):

        teacher_tokens = self.teacher_tokenizer(self.input[idx], max_length=450, padding = 'max_length', truncation = True)
        studnet_tokens = self.student_tokenizer(self.input[idx], max_length=450, padding = 'max_length', truncation = True)

        inputs_ids_teacher = teacher_tokens['input_ids']
        inputs_ids_student = studnet_tokens['input_ids']

        attn_mask_teacher = teacher_tokens['attention_mask']
        attn_mask_student = studnet_tokens['attention_mask']

        label = torch.tensor(self.label[idx]).long() - 1

        return {
            "inputs_ids_teacher": torch.tensor(inputs_ids_teacher),
            "inputs_ids_student": torch.tensor(inputs_ids_student),
            "attn_mask_teacher": torch.tensor(attn_mask_teacher),
            "attention_mask_std": torch.tensor(attn_mask_student),
            "label": label
        }

train_dataset = MyCustomDataset(train_datasets)
test_dataset =  MyCustomDataset(test_datasets)
valid_dataset =  MyCustomDataset(valid_datasets)

train_data = DataLoader(train_dataset, batch_size=32, shuffle = True)
valid_data = DataLoader(valid_dataset, batch_size=8, shuffle = True)
test_data  = DataLoader(valid_dataset, batch_size=8, shuffle = False)

"""# **Training Loop**"""

def DistilationLoss(teacher_logits, student_logits, label):

    alpha: float = 0.2

    teacher_norm_logits = torch.log_softmax(teacher_logits, dim = -1)
    student_norm_logits = torch.log_softmax(student_logits, dim = -1)

    teacher_probs = torch.gather(teacher_norm_logits, -1, label)
    student_probs = torch.gather(student_norm_logits, -1, label)

    kl_div = teacher_probs - student_probs

    kl_div = kl_div.squeeze(-1).unsqueeze(0)

    distil_loss = torch.sum(kl_div, dim = -1)

    student_model_loss = nn.CrossEntropyLoss()(student_logits, label.squeeze(-1))

    loss = alpha * student_model_loss + (1.0 - alpha) * distil_loss

    return loss

wandb.login(key = "ee0683c540665b9502ef44b224c3796a0e8a907f")

# -------------------- CONFIG -------------------- #
epochs: int = 12
lr: float = 5e-5
temperature: float = 0.2

device = "cuda" if torch.cuda.is_available() else "cpu"
optimizer = torch.optim.AdamW(student_model.parameters(), lr=lr)

# -------------------- WANDB INIT -------------------- #
wandb.init(
    project="distillation-training",
    config={
        "epochs": epochs,
        "learning_rate": lr,
        "temperature": temperature,
    }
)

student_model = student_model.to(device)
teacher_model = teacher_model.to(device)
teacher_model.eval()  # teacher always frozen


# ================== TRAINING LOOP ================== #
for epoch in range(epochs):

    # -------------------- TRAIN -------------------- #
    student_model.train()
    train_loss_sum = 0.0
    train_total = 0

    pbar = tqdm(train_data, desc=f"Epoch {epoch+1}/{epochs} [Train]")

    for batch in pbar:

        # unpack
        input_teacher  = batch['inputs_ids_teacher'].to(device)
        input_student  = batch['inputs_ids_student'].to(device)
        attn_teacher   = batch['attn_mask_teacher'].to(device)
        attn_student   = batch['attention_mask_std'].to(device)
        label          = batch['label'].to(device)

        # forward
        with torch.no_grad():
            teacher_logits = teacher_model(
                input_ids=input_teacher,
                attention_mask=attn_teacher
            )['logits'] / temperature

        student_logits = student_model(
            input_ids=input_student,
            attention_mask=attn_student
        )['logits'] / temperature

        # loss
        loss = DistilationLoss(teacher_logits, student_logits, label.unsqueeze(-1))

        # backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # accumulate average
        batch_size = input_student.size(0)
        train_loss_sum += loss.item() * batch_size
        train_total += batch_size

        # tqdm update
        pbar.set_postfix(loss=loss.item())

        # wandb log per step
        wandb.log({"train_step_loss": loss.item()})


    # epoch-level train loss
    train_epoch_loss = train_loss_sum / train_total


    # -------------------- VALIDATION -------------------- #
    student_model.eval()
    val_loss_sum = 0.0
    val_total = 0

    vbar = tqdm(valid_data, desc=f"Epoch {epoch+1}/{epochs} [Valid]")

    with torch.no_grad():
        for batch in vbar:
            input_teacher  = batch['inputs_ids_teacher'].to(device)
            input_student  = batch['inputs_ids_student'].to(device)
            attn_teacher   = batch['attn_mask_teacher'].to(device)
            attn_student   = batch['attention_mask_std'].to(device)
            label          = batch['label'].to(device)

            teacher_logits = teacher_model(
                input_ids=input_teacher,
                attention_mask=attn_teacher
            )['logits'] / temperature

            student_logits = student_model(
                input_ids=input_student,
                attention_mask=attn_student
            )['logits'] / temperature

            loss = DistilationLoss(teacher_logits, student_logits, label.unsqueeze(-1))

            # accumulate
            batch_size = input_student.size(0)
            val_loss_sum += loss.item() * batch_size
            val_total += batch_size

            vbar.set_postfix(val_loss=loss.item())

    val_epoch_loss = val_loss_sum / val_total


    # -------------------- WANDB LOGGING (epoch) -------------------- #
    wandb.log({
        "epoch": epoch + 1,
        "train_epoch_loss": train_epoch_loss,
        "val_epoch_loss": val_epoch_loss,
        "lr": optimizer.param_groups[0]["lr"]
    })

    print(f"\nEpoch {epoch+1}/{epochs} | Train Loss: {train_epoch_loss:.4f} | Val Loss: {val_epoch_loss:.4f}\n")


wandb.finish()

torch.save(student_model.state_dict(), "weights.pt")

device = "cuda" if torch.cuda.is_available() else "cpu"
import os
os.path.getsize("weights.pt")

student_model.load_state_dict(torch.load("weights.pt"))

