# -*- coding: utf-8 -*-
"""Prunning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vSfDiD3SL4EcL1-q4DELXzAInzsJlRHe
"""

import torch
import torch.nn as nn
from tqdm import tqdm
import numpy as np
import wandb
import torch.nn.functional as F
import matplotlib.pyplot as plt
from dataclasses import dataclass
from torchvision import datasets, transforms
from torch.utils.data import random_split, Subset

wandb.login(key = "")

@dataclass

class Config:

  lr = 5e-3
  epochs: int = 3
  device = "cuda" if torch.cuda.is_available() else "cpu"

config = Config()

# Transformation (convert to tensor & normalize)
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

# Download + load training dataset
train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)


# Download + load test dataset
valid_datas = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
valid_indices = torch.randperm(len(valid_data))[:10000]
valid_data = Subset(valid_datas, valid_indices)

test_indices = torch.randperm(len(valid_datas))[:5000]
test_data = Subset(valid_datas, test_indices)

# Create data loaders
train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)
valid_data = torch.utils.data.DataLoader(valid_data, batch_size=12, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)

class Model(nn.Module):

    def __init__(self, config):

        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=7)
        self.conv2 = nn.Conv2d(32, 48, kernel_size=3)
        self.norm1 = nn.LayerNorm(256, 0.00001)
        self.pool = nn.MaxPool2d(2, 2)

        self.relu = nn.LeakyReLU(0.0001)

        self.ffnn1 = nn.Linear(768, 256)
        self.cls_head = nn.Linear(256, 10)

    def forward(self, x, target = None):

        B, C, H, W = x.size()
        feature_map1 = self.pool(self.relu(self.conv1(x)))
        feature_map2 = self.pool(self.relu(self.conv2(feature_map1)))

        flattened = feature_map2.reshape(B, -1)

        feature1 = self.norm1(self.relu(self.ffnn1(flattened)))
        logits = self.cls_head(feature1)

        loss = None
        actual = None

        if target is not None:
          loss = nn.CrossEntropyLoss()(logits, target.reshape(-1))
          actual = torch.argmax(logits, dim = -1)
          total_correct = torch.sum((actual.reshape(-1) == target.reshape(-1)).long()).item()
          total = len(actual.reshape(-1))
          accuracy = total_correct/total

        return logits, loss, accuracy

model = Model(config=config).to(config.device)

print(f'''\n total-parameters: {sum(p.numel() for p in model.parameters())/1e6 :.5f} Millions\n''')

optimizers = torch.optim.AdamW(model.parameters(), lr = config.lr)

run = wandb.init(
    name = "threshold-training-prunning",
    config=vars(config),
)

valid_step = 400

for epoch in tqdm(range(config.epochs), desc = f"Epochs "):

    run.log({"epochs": epoch+1})

    valid_init = 0
    model.train()
    for i, batch in enumerate(train_loader):
      valid_init +=1
      X, y = batch
      X, y = X.to(config.device), y.to(config.device)
      cls, loss, accuracy_score = model(X, y)

      if i % 300==0:
        run.log({"train-batch-loss": loss, "batch-train-accuracy": accuracy_score})

      optimizers.zero_grad()
      loss.backward()
      optimizers.step()

      # layer-wise-threshold-prunning.
      for name, modules in model.named_modules():
        if isinstance(modules, nn.Linear):
          mask = (torch.abs(modules.weight.data) > 0.001).long()
          modules.weight.data = mask * modules.weight.data

      if (valid_init+1)%valid_step==0:
        model.eval()
        valid_loss = 0
        valid_size = 0
        accuracy_score = 0
        with torch.no_grad():
          for i, batch in enumerate(valid_data):
            X, y = batch
            X, y = X.to(config.device), y.to(config.device)
            _, loss, accuracy_score = model(X, y)
            if i % 100==0:
              run.log({"per-batch-valid-accuracy": accuracy_score})
            valid_loss +=loss.item()
            valid_size +=X.size(0)

          batch_valid_loss = valid_loss / valid_size

          run.log({"valid-after-batch-loss": batch_valid_loss})

run.finish()

'''MANUAL-THRESHOLD-PRUNNING'''

# for name, modules in model.named_modules():
#   if isinstance(modules, nn.Linear):
#     mask = (torch.abs(modules.weight.data) > 0.05).long()
#     modules.weight.data = mask * modules.weight.data

for name, modules in model.named_modules():
  if isinstance(modules, nn.Linear):
    print(modules.weight.data)

for k, batch in enumerate(test_loader):
  X, y = batch
  X, y = X.to(config.device), y.to(config.device)
  logits, loss, acc = model(X, y)

  #print(f'{torch.argmax(logits, dim = -1).item()} | {y.item()}')
print(f"Loss: {loss:.2f}, Accuracy-Score: {acc}")

for name, modules in model.named_modules():

  if isinstance(modules, nn.Linear):
    weights = modules.weight.data
    plt.plot(weights.reshape(-1).cpu().detach(), color = 'black')
    print(f"\n\nLayer: {name}")
    plt.grid(True, alpha = 0.5)
    print(f'MEAN: {torch.mean(weights.reshape(-1).cpu().detach()).item():.5f}')
    plt.show()